{
  "hash": "c68b86ed13ddb6eb9824709b6a384ac5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Power Analysis in R\"\ndescription: |\n  How can you tell if the two groups are different, I mean different enough to\n  make a decision?\nauthor: Ben Woodard\ndate: 2025-11-16\nimage: \"/images/bayes_power_analysis.png\"\n---\n\n## Setup\n\nI need to run a test that will have a good chance of getting results.  How long should I run it for?\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"rstanarm\")  # if needed\nlibrary(rstanarm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is rstanarm version 2.32.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n  options(mc.cores = parallel::detectCores())\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\n# library(ggplot2)  # optional, for plotting\n\nset.seed(1234)\n```\n:::\n\n\n## 2. Data generator: simulate an A/B test for a given number of days\n\nKey Terms: \n\n* baseline_p = control conversion rate\n* lift = relative lift in treatment (e.g. 0.1 = +10% relative)\n* visitors_per_day = total visitors per day (split 50/50)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_ab_binomial_days <- function(\n  days,\n  visitors_per_day = 5000,\n  baseline_p       = 0.05,\n  lift             = 0.10   # 10% relative lift\n) {\n  days = 14\n  n_control_per_day  <- floor(visitors_per_day / 2)\n  n_treat_per_day    <- visitors_per_day - n_control_per_day\n  \n  n_control <- n_control_per_day * days\n  n_treat   <- n_treat_per_day   * days\n  \n  p_control <- baseline_p\n  p_treat   <- baseline_p * (1 + lift)\n  \n  y_control <- rbinom(1, n_control, p_control)\n  y_treat   <- rbinom(1, n_treat,   p_treat)\n  \n  data.frame(\n    group    = factor(c(\"Control\", \"Treatment\"),\n                      levels = c(\"Control\", \"Treatment\")),\n    successes = c(y_control, y_treat),\n    failures  = c(n_control - y_control,\n                  n_treat   - y_treat)\n  )\n}\n```\n:::\n\n\n## 3. Fit Bayesian logistic model in rstanarm\n\nWe use a simple binomial regression: cbind(successes, failures) ~ group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_bayes_ab <- function(dat,\n                         iter   = 1000,\n                         chains = 2) {\n  stan_glm(\n    cbind(successes, failures) ~ group,\n    data   = dat,\n    family = binomial(),\n    prior           = normal(0, 1, autoscale = TRUE),\n    prior_intercept = normal(0, 5, autoscale = TRUE),\n    chains = chains,\n    iter   = iter,\n    cores  = 1,   # keep it simple & robust\n    refresh = 0\n  )\n}\n```\n:::\n\n\n## 4. Bayesian decision rule: “Is treatment better?”\n\nWe use the posterior difference in conversion rates (not just the log-odds).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompute_decision <- function(fit,\n                             prob_threshold = 0.95) {\n  nd <- data.frame(\n    group = factor(c(\"Control\", \"Treatment\"),\n                   levels = c(\"Control\", \"Treatment\"))\n  )\n  \n  # Posterior predicted conversion rate for each group\n  pred <- posterior_epred(fit,\n                            newdata  = nd)  # on probability scale\n  \n  # pred: draws x 2 (Control, Treatment)\n  diff_draws <- pred[, 2] - pred[, 1]        # Treatment - Control\n  \n  post_prob <- mean(diff_draws > 0)          # P(Treatment > Control)\n  list(\n    posterior_prob = post_prob,\n    success        = post_prob >= prob_threshold\n  )\n}\n```\n:::\n\n\n## 5. Simulation-based “power” for a given duration\n\nFor a given number of days, we:\n\nSimulate many fake tests.\n\nFit the Bayesian model each time.\n\nApply the decision rule.\n\nEstimate how often we’d “declare a win”.\n\nThat frequency is your Bayesian power for that duration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayes_power_for_days <- function(\n  days,\n  visitors_per_day = 5000,\n  baseline_p       = 0.05,\n  lift             = 0.10,\n  sims             = 100,    # bump up later (e.g. 500–1000)\n  prob_threshold   = 0.95,\n  iter             = 1000,\n  chains           = 2\n) {\n  results <- map_dfr(1:sims, function(i) {\n    dat <- simulate_ab_binomial_days(\n      days            = days,\n      visitors_per_day = visitors_per_day,\n      baseline_p      = baseline_p,\n      lift            = lift\n    )\n    \n    fit <- fit_bayes_ab(dat, iter = iter, chains = chains)\n    dec <- compute_decision(fit, prob_threshold = prob_threshold)\n    \n    tibble(\n      sim            = i,\n      posterior_prob = dec$posterior_prob,\n      success        = dec$success\n    )\n  })\n  \n  power_est <- mean(results$success)\n  mean_prob <- mean(results$posterior_prob)\n  \n  list(\n    days                = days,\n    power_estimate      = power_est,\n    mean_posterior_prob = mean_prob,\n    results             = results\n  )\n}\n```\n:::\n\n\n## 6. Estimate required duration: run over a grid of days\n\nNow we check, say, 7, 10, 14, 21, 28 days and see where power crosses your target (e.g. 0.8).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndays_grid <- c(7, 14, 21, 28)\n\npower_grid <- map_dfr(days_grid, function(d) {\n  \n  res <- bayes_power_for_days(\n    days             = d,\n    visitors_per_day = 5000,\n    baseline_p       = 0.05,\n    lift             = 0.10,   # 10% relative effect\n    sims             = 800,    # increase to 500+ for production\n    prob_threshold   = 0.95\n  )\n  \n  # Compute credible interval of the success indicator (power)\n  ci_lower <- quantile(res$results$posterior_prob, 0.05)\n  ci_upper <- quantile(res$results$posterior_prob, 0.95)\n  \n  tibble(\n    days                = d,\n    power_estimate      = res$power_estimate,\n    mean_prob           = mean(res$results$posterior_prob),\n    ci_lower            = ci_lower,\n    ci_upper            = ci_upper\n  )\n})\n\npower_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n   days power_estimate mean_prob ci_lower ci_upper\n  <dbl>          <dbl>     <dbl>    <dbl>    <dbl>\n1     7          0.906     0.985    0.920        1\n2    14          0.904     0.980    0.897        1\n3    21          0.911     0.981    0.903        1\n4    28          0.912     0.981    0.904        1\n```\n\n\n:::\n:::\n\n\n\nThen you’d say something like:\n\n“To detect a 10% lift with P(treatment > control | data) ≥ 0.95 about 80–90% of the time, I need around 14–21 days at 5k visitors/day and 5% baseline.”\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(power_grid, aes(x = days, y = power_estimate)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +\n  labs(\n    title = \"Bayesian Power vs Test Duration\",\n    subtitle = \"With 90% credible intervals from simulation uncertainty\",\n    x = \"Test Length (days)\",\n    y = \"Bayesian Power (P(p_treat > p_control) ≥ threshold)\"\n  ) +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Conclusion\n\nUsing a Bayesian approach to power analysis brings many more capabilities that the Frequentist approach does not.  Some of these differences are more beneficial than others but overall, utilizing the Bayesian approach to power analysis is a great option.  \n\n\n| Capability                                        | Frequentist | Bayesian         |\n| ------------------------------------------------- | ----------- | ---------------- |\n| Connect power to an actual business decision      | ❌           | ✅                |\n| Incorporate prior information                     | ❌           | ✅                |\n| Use a distribution of plausible effect sizes      | ❌           | ✅                |\n| Simulate adaptive/sequential stopping             | ❌ (hard)    | ✅ (easy & valid) |\n| Directly answer “probability treatment is better” | ❌           | ✅                |\n| Model realistic data-generating processes         | Limited     | Excellent        |\n| Handle hierarchical/geo experiments               | Hard        | Natural          |\n| Provide interpretable outputs                     | ❌           | ✅                |\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}