{
  "hash": "9c45d0e2a439fd38e1b8bde41aaf41ab",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Basic Linear Regression Model Example\"\ndescription: |\n  The workhorse of all data science work is the basic linear regression model. It has the capability to change the direction of a business like a rudder on a boat but like a rudder, it is often overlooked by most practicioners until other more 'advanced' tactics are tried.\ncategories: [r, bayesian, marketing]\nauthor: Ben Woodard\ndate: 2025-11-16\nimage: \"/images/hill_function_example.png\"\n---\n\nHere is an example of a basic bayesian linear regression model in R and stan.\n\nSimulating the data is pretty straight forward.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nN <- 100\nx <- rnorm(N, 0, 1)\n\nalpha_true <- 2.5\nbeta_true  <- 0.8\nsigma_true <- 1.0\n\ny <- alpha_true + beta_true * x + rnorm(N, 0, sigma_true)\n\ndf <- data.frame(x, y)\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            x        y\n1 -0.56047565 1.341213\n2 -0.23017749 2.572742\n3  1.55870831 3.500275\n4  0.07050839 2.208864\n5  0.12928774 1.651812\n6  1.71506499 3.827024\n```\n\n\n:::\n:::\n\n\nNow that we have the data we want to create a linear regression model that shows the results of \n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm(y ~ x, data = df)\nsummary(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9073 -0.6835 -0.0875  0.5806  3.2904 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.39720    0.09755  24.574  < 2e-16 ***\nx            0.74753    0.10688   6.994  3.3e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9707 on 98 degrees of freedom\nMultiple R-squared:  0.333,\tAdjusted R-squared:  0.3262 \nF-statistic: 48.92 on 1 and 98 DF,  p-value: 3.304e-10\n```\n\n\n:::\n:::\n\n\nKey parts of summary(lm_fit):\n\n* Estimate: point estimates of intercept and slope\n* Std. Error: estimated standard error of each coefficient\n* t value, Pr(>|t|): classical hypothesis test for each coefficient\n* Residual standard error: estimate of σ\n* Multiple R-squared: variance explained\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                2.5 %    97.5 %\n(Intercept) 2.2036098 2.5907841\nx           0.5354312 0.9596255\n```\n\n\n:::\n:::\n\n\nInterpretation (frequentist):\n\nIf we repeated this estimation procedure infinitely many times,\n95% of the constructed intervals would contain the true parameter.\n\n(Contrast: in Bayesian, you’d say “there’s a 95% probability the parameter lies in this interval, given data + priors.”)\n\n## Basic Diagnostic Plots\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(lm_fit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nYou get:\n\n* Residuals vs Fitted (linearity, homoskedasticity)\n* Normal Q-Q (normality of residuals)\n* Scale-Location\n* Residuals vs Leverage (influence)\n\n## Predictions with confidence & prediction intervals\n \n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- data.frame(x = seq(-2, 2, length.out = 30))\n\n# Confidence interval for the *mean* response\npred_conf <- predict(lm_fit, newdata = new, interval = \"confidence\", level = 0.95)\n\n# Prediction interval for a *new* observation\npred_pred <- predict(lm_fit, newdata = new, interval = \"prediction\", level = 0.95)\n\nhead(pred_conf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit       lwr      upr\n1 0.9021402 0.4187310 1.385549\n2 1.0052475 0.5485212 1.461974\n3 1.1083549 0.6779769 1.538733\n4 1.2114623 0.8070330 1.615892\n5 1.3145696 0.9356071 1.693532\n6 1.4176770 1.0635953 1.771759\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(pred_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit        lwr      upr\n1 0.9021402 -1.0839403 2.888221\n2 1.0052475 -0.9745075 2.985003\n3 1.1083549 -0.8654881 3.082198\n4 1.2114623 -0.7568858 3.179810\n5 1.3145696 -0.6487041 3.277843\n6 1.4176770 -0.5409462 3.376300\n```\n\n\n:::\n:::\n\n\n* \"confidence\" = uncertainty about E[y | x]\n* \"prediction\" = uncertainty about a new y at that x (includes residual noise, so wider)\n \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\npred_df <- cbind(new, as.data.frame(pred_conf))\nnames(pred_df) <- c(\"x\", \"fit\", \"lwr\", \"upr\")\n\n# ggplot(df, aes(x, y)) +\n#   geom_point(alpha = 0.7) +\n#   geom_line(data = pred_df, aes(x, fit)) +\n#   geom_ribbon(\n#     data = pred_df,\n#     aes(x = x, ymin = lwr, ymax = upr),\n#     alpha = 0.2\n#   ) +\n#   labs(\n#     title = \"Frequentist Linear Regression (lm)\",\n#     subtitle = \"Fitted line with 95% confidence band for mean response\"\n#   )\n```\n:::\n\n\n\n7. Quick contrast with your Bayesian version\nUsing the same simulated data:\n\n* Frequentist (lm):\n  * Coefficients: point estimates + SE\n  * 95% CIs: coverage interpretation over repeated samples\n  * p-values: H₀ tests (β = 0, etc.)\n\n* Bayesian (stan_glm):\n  * Coefficients: full posterior distributions\n  * 95% intervals: “credible intervals” (probability statements given model + priors)\n  * No p-values; you might instead look at Pr(β > 0) or posterior density.\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}